# Ход работы

*В данном документе представлен общий ход работы наиболее развернуто.* 

Основной задачей было нахождение подходов к файнтюнингу модели распознавания речи для применения в сфере IT.

## Выбор модели
На основе [сравнения](https://alphacephei.com/nsh/2023/01/22/russian-models.html) открытых моделей для распознавания речи на различных датасетах было выделено три наиболее интересных варианта:
1. [Vosk](https://alphacephei.com/vosk/models)
2. [NVIDIA Conformer-Transducer Large (RU)](https://huggingface.co/nvidia/stt_ru_conformer_transducer_large)
3. [Whisper](https://openai.com/research/whisper)

При дальнейшем изучении выбор пал на модель Whisper от OpenAI, так как: 
1. она легко обучается(в отличие от Vosk, основанной на системе инструментов Kaldi) (лектор [курса](https://www.youtube.com/playlist?list=PLEwK9wdS5g0oE9htlwY-WarI5_jeH818j) по обработки аудио от ВШЭ ФКН, упоминает, что "кто с Kaldi работал, тот в цирке не смеется". В условиях ограниченного времени, я решил поверить ему на слово и не проверять на себе.)
2. умеет из коробки включать в распознавание русского языка латинские символы, что позволяет ее дообучать, а не переучивать с нуля (в отличие от модели от Nvidia). 

Кажется, при создании MVP, лучше не искать дополнительных сложностей, особенно, если они не сулят значительного улучшения качества.

На данный момент использовалась модель Whisper размера Small (третья по величине из пяти), так как обучение старших моделей не поместилось 15gb GPU, которые доступны бесплатно на платформах Kaggle или Colab. Если будут хорошие результаты при обучении модели small, можно будет арендовать на несколько дней сервер и поэкспериментировать с большой моделью, используя обкатанную технологию.

## Определение качества модели
### Тестовые датасеты
#### Изначальные версии тестовых датасетов
Чтобы наиболее объективно оценить результаты экспериментов, делались оценки на 5 разных датасетах. 
* основной датасет - test-split датасета на основе тематических ютуб-видео (митапы, мок-интервью, интервью на каналах про IT). собран и размечен вручную мной (подробнее про использование ютуб-видео дальше)
* еще 3 это test-сплиты, использовавшихся при обучении открытых датасетов: Sova Audiobooks, Common Voice, Golos
* и еще один - датасет записей звонков из [набора Silero](https://github.com/snakers4/open_stt) был добавлен для большей объективности. Ничего похожего на данный датасет в обучающей выборке не было. 

В будущем хотелось бы создать единый датасет, который был бы максимально сбалансирован и близок к планируемой сфере использования. Однако, как мне кажется, это можно сделать только когда проект начнет работать, собирая данные с первых пользователей. Пока этого нет, я решил использовать ряд датасетов, чтобы на основе их пытаться составить более полное представление о свойствах модели при ее дообучении.

#### Аугментирование тестовых датасетов
Так как часть датасетов обладали достаточно хорошим качеством, а задача должна была включать работу с шумами и данные на обучении аугментировались шумами, то рассчет метрик на выбранных датасетах показался мне недостаточно инфоративным, поэтому были созданы еще аугментированные версии датасетов. 

Использовались следующие аугментации:
* добавление шумов окружающей среды из [датасета](ttps://github.com/karolpiczak/ESC-50) на задний план (стук клавиатуры, кашель, шум улицы, и тд) 
* ApplyImpulseResponse (симуляция акустики различных помещений).  Эффекты взяты из набора [MIT](http://mcdermottlab.mit.edu/Reverb/IR_Survey.html)
* наложение гаусового шума (разновидность белого шума)

При обучении использовался более широк набор аугментации, однако для тестов были выбраны те аугментации, которые не искажают сам звук.

Таким образом всего оценивалось качество на 10 датасетах (5 изначальных + 5 аугментированных).

### Выбор метрик
Были выбраны следующие метрики:
* wer -  расстояние Левенштейна на уровне слов, деленное на количество слов с правильном ответе (чем ниже, тем лучше)  
* сer -  расстояние Левенштейна на уровне символов, деленное на количество слов в правильном ответе(чем ниже, тем лучше)  
* keywords_rate - доля верно распознанных терминов (чем выше, тем лучше) (использовалось только для ютуб-датасета)

## Особенности обучения
Параметры обучения были подобраны в ходе начальных экспериментов, а после зафиксированы. Эксперименты отличались только составом обучающей выборки. 

При обучении применялись следующие аугментации:
* замедление/ускорение (0.8x - 1.25x)
* наложение гаусового шума (разновидность белого шума)
* уменьшение/увеличение громкости
* Pitch Shifting (изменение тембра)
* ApplyImpulseResponse (симуляция акустики различных помещений) Эффекты взяты из набора [MIT](http://mcdermottlab.mit.edu/Reverb/IR_Survey.html)
* BandPassFilter (фильтр частот)
* ClippingDistortion (Дисто́ршн — звуковой эффект, достигаемый искажением сигнала путём его «жёсткого» ограничения по амплитуде, или устройство, обеспечивающее такой эффект.)
* наложение шумовых звуков (звуки окружающей среды из (датасета)['https://github.com/karolpiczak/ESC-50'](пылесос, шум машин, плач ребенка, и тд, всего 50 категорий))

-----
-----

На этом фиксированная часть заканчивается и начинается экспериментирование с данными. Часть экспериментов была доведена до результатов, для части проделана лишь подготовительная работа. Однако вместе они, как мне кажется, являются неплохим началом для моей будущей работы над проектом.

-----

## Ход работы

Термины можно обрабатывать по разному: 
* изначально распознавать латиницей в русском тексте
* сначала распознавать кириллическую транскрипцию, а потом восстанавливать написание латиницей. 

В качество наиболее предпочтительного варианта рассматривался вариант изначального распознавания латиницей.

-----
Далее идет текстовое описание интуиции экспериментов и их результатов. Увидеть численные результаты основных экспериментов можно [здесь](https://docs.google.com/spreadsheets/d/1qVgNXQ9315JK8VqBGJyCbRVeZYzFEwyU4HxO59FsNjg/edit?usp=sharing).

-----

#### Открытые датасеты

После изучения результатов распознавания изначальной версии Whisper Small стало понятно, что улучшения требует не только распознавание терминов, но и в принципе русского языка. 
Было решено начать с фантюнинга на открытых датасетах. Были выбраны следующие:
1. Golos (crowd) от Сбера (часть)
2. Sova dataset (audiobooks) от sova.ai
3. CommonVoice от Mozilla  

(прим. суммарно около 1200 часов)  
(прим. Golos и Sova не применялись при обучении командой OpenAI, CommonVoice - применялся)
 
Качество распознавания улучшилось на всех датасетах кроме основного. Ошибка на ютуб-датасете увеличилась за счет стремительного ухудшения распознавания терминов. Модель стала обучаться под русский язык и при этом забывать английский. Термины стали распознаваться кириллическими символами и при этом не то чтобы точно.

-----

#### Youtube

Одной из идей где можно взять данные из целевой сферы было использование данных с платформы Youtube. Можно взять видео из нужной сферы (митапы, мок-интервью, интервью с гостями из данной сферы). Автоматические субтитры есть почти на всех видео, и, кажется, они неплохого качества. Даже часть специальной лексики правильно распознают (не всю, конечно, но лучше, чем ничего). Часть лексики распознается кириллицей (например, 'докер' часто пишется кириллицей), однако часть распознается латиницей.

Также стоит отметить, что есть программы, которые позволяют скачивать необходимые файлы (аудио и субтитры) с Ютуба целыми плейлистами или даже каналами, поэтому, имея автоматический обработчик, можно собрать сколько угодно данных.
Мной был написан такой обработчик, который из папки, содержащей файлы субтитров и аудио-файлы, создает датасет на платформе HuggingFace. Для первых экспериментов собрано 150 часов данных, что для файнтюнинга вполне достаточно.

1 час был выделен в тестовую выборку и размечен вручную. Сравнение ручной разметки с автоматическими субтитрами показало ошибку wer=0.145. Это несколько меньше, чем даже ошибка самой большой модели Whisper(wer=0.187), не говоря уже о модели Small, которую я собирался обучать. В связи с этим я решил оставить данные с Youtube в обучающей выборке.

Качество распознавание терминов стало лучше, чем при обучении на открытых датасетах, но все еще оставалось хуже, чем изначально. На открытых датасетах качество осталось сильно лучше изначального. Таким образом включение ютуб-датасета оказалось целесообразным. 

В связи с тем, что качество распознавания терминов все еще было ниже изначального, я решил добавить данных из англоязычного Ютуба. Качество субтитров там выше, больше ручных субтитров, все термины точно распознаются латиницей.

Это улучшило качество распознавания терминов, увилив долю верно распознанных терминов с 26% до 32% по сравнению с бейзлайном. 

Однако возникла неожиданная для меня ситуация:
* Дообученная версия показывает себя лучше на 4ех фйл-рускоязычных датасетах по сравнению с бейзлайном.					
* Качество распознавание англоязычных слов в юутб-датасете у дообученной модели лучше, чем  у бейзлайна.					
* При этом  общее качество на ютуб-датасете у дообученной модели чуть ниже. 	

Почему так происходит пока выяснить не удалось.

Несмотря на это, данная модель кажется мне более менее удачной. Возможно, при другом соотношении данных из различных источников можно получить более очевидное улучшение по всем показателям.

-----

#### Текстовые данные

Однако все таки хотелось бы улучшить качество обучающего датасета с целью улучшения результатов обучения. Первой идеей было доразметить автоматические субтитры на Толоке, однако в ходе разметки тестовой выборки я понял, что это очень трудозатратный процесс. По моему опыту исправление ошибок занимает в несколько раз больше времени, чем количество материала, которое надо разметить. Учитывая, что такую разметку, надо еще и перепроверять, это слишком дорого.

В связи с этим появилась идея зайти с другой стороны: попросить людей на Толоке наговорить нужный текст, а не доразметить существующее аудио. Но откуда взять столько текстовых данных?

Я решил спарсить Хабр. Всего для эксперимента спарсил около 5000 статей. На данный момент сфокусировался на написанных латиницей, хотя и среди кириллических слов наверяка есть редкие, которые было бы полезно разметить. Далее я
1. посчитал число вхождений каждого англоязычного слова. 
2. Удалил слова, которые входят в 333 тысячи самых распространенных слов английскоязычного интернета (ушли такие слова как Microsoft, Android, IOS, и тд) (кажется, их распознать можно и без допразметки.)
3. удалил слова, встретившиеся меньше 3 раз и в которых меньше 5 букв. 
Субъективно, получился достаточно хороший словарь. 
4. после этого выделил предложения, содержащие такие слова, для последующей разметки голосом.

Разметить на Толоке достаточно предложений для обучающей выборки мне явно не позволяет бюджет. Однако, в планах было разметить хотя бы часть для тестовой выборки. К сожалению, столкнулся с тем, что Толока просит публиковать соглашение об обработке персональных данных, так как голос человека является частью таких данных. Времени до сдачи проекта оставалось мало - разобраться что это за соглашение у меня не получилось. Поддержка Толоки предоставить шаблон не смогла.

Однако есть еще один вариант - сделать озвучку с помощью моделей text-to-speech. Это нормальный метод, современные TTS модели обладают достаточным качеством, чтобы на них можно было обучать STT. Об этом говорится, например вот в [этой](https://arxiv.org/abs/1811.02050) статье от Google Research. 

Попробовав генерировать озвучку с помощью TTS [модели от Facebook](https://huggingface.co/facebook/tts_transformer-ru-cv7_css10) я обнаружил, что английски слова в русском тексте модель читает с явным английским акцентом, что может мешать способности модели узнавать эти слова при другом произношении. Также, он может не знать как принято произносить слова, и произносить их согласно правилам английского языка. Например 'nosql' модель произносит как 'носкл' вместо ожидаемоего ~'ноу-эс-ку-эл'.

Решить эту проблему, кажется, можно только создав словарь, в котором собранные англоязычные термины имели транскрипции из кириллицы. Кажется абсолютного число терминов покрывается словарем в несколько тысяч слов. Разметить такое количество не так долго и дорого. Имея же такой словарь можно будет:
1. генерировать тексты гораздо лучшего качества с помощью TTS моделей
2. снизить стоимость при разметке на Толоке и повысить ее качество, так как толокеры будут точно читать правильно
3. использовать модели только с кириллическим токенизатором, без обучения с нуля
4. такие данные не будут входить в конфликт с датасетом Голос при обучении, в котором все англоязычные слова приведены к кириллице

Проблема будет заключаться только в том, что ошибка модели на один символ уже не позволит сметчить кириллическое предсказание с исходным словом на английском, которое мы хотим показывать в итоговой расшифровке. Но, наверное, эту проблемы можно постараться решить либо эвристиками, либо дополнительной маленькой языковой моделькой. Но это уже совсем другая история, которой, я надеюсь, я займусь во время учебы в магистратуре ИТМО :)