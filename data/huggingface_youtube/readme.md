# YoutubeAsrDatasetCreator

YoutubeAsrDatasetCreator позволяет создавать и обновлять датасет для задачи распознавания речи на основе видео 
с платформы Youtube. 

В качестве расшифровок использовались субтитры, автоматически сгенерированные платформой Youtube.
Датасеты создаются на платформе HuggingFace.

(!) на данный момент код не является качественным, хотя он полностью рабочий) в процессе отладки прикручивались всякие костыли, чтобы быстрее протестировать идею о пользе такого решения. к концу курса, если останется время, постараюсь привести его к более приемлимому виду)


## Данные
При разработке использовались данные, собранные с помощью двух платформ.
* mp3 файлы скачивались с помощью приложения 4K Video Downloader: https://www.4kdownload.com/ru/
* srt файлы субтитров скачивались с помощью сервиса DownSubs: https://downsub.com/

Сервис 4K Video Downloader был выбран для скачивания mp3, так как он имеет достаточно широкий функционал, позволяющий
в платных подписках скачивать видео целыми плейлистами или даже каналами. Однако он не позволяет скачивать субтитры без
видео. Для этого был использован сервис DownSubs. На данный момент субтитры для скачанных аудио скачивались вручную, но
в будущем данный процесс можно автоматизировать, например с помощью Selenium.

Стоит отметить, что сервисы по разному обрабатывают названия файлов.
1) Если в названии ролика встретились символы, недопустимые для имени файла, 4K Video Downloader заменяет их на пробелы,
 аDownSubs - на нижние подчеркивания.
2) Сервис DownSubs добавляет в название файла информацию о языке субтитров и свой тег.
Данные различия обрабываются автоматически, приводя названия файлов к одинаковым. (методы normalize_names, merge_names)

## Создание датасета
Чтобы создать датасет нужно заполнить dataset_config.py
1) добавить путь к папке с mp3 и srt файлами
2) указать путь где можно хранить нарезанные данные в процссе создания датасета
3) указать информацию об аккаунте huggingface
4) задать название датасета
5) задать параметр train/test сплита
6) также можно в отдельную папку выделить часть контрольных данных. эти данные будут 
использоваться для контрольных проверок. их необходимо отдельно доразметить (либо самостоятельно, либо с помощью краудсорсинговых платформ)

Хотелось бы иметь возможность регулярно добавлять новые данные. Однако я не нашел возможности догружать 
данные в существующий датасет без того, чтобы скачивать его целиком, модифицировать и загружать обратно. 
В связи с этим на каждой итерации будут создаваться новые датасеты. В библиотеке 
datasets есть возможность формировать батчи из нескольких датасетов, так что это не проблема.

Я также не нашел способа получить все датасеты пользователя, чтобы выбрать автоматически нужные для формирования батчей. 
К датасету можно обратиться только по заранее известной ссылке, поэтому названия всех созданных датасетов необходимо хранить
самостоятельно. Такая информация будет собираться в файле created_datasets.csv

Также я хочу сохранять названия роликов, которые были уже обработаны, чтобы не добавлять один и тот же материал по 
несколько раз. Для этого создается файл used_videos.csv

Если на данный момент файлов used_videos.csv и created_datasets.csv нет в директории проекта, то они создадутся.
